# Code in R (v4.3.1) for the paper: "Psychometric validation of the Hospital Stress Questionnaire"

# Load packages and survey response data
install.packages("tidyverse")
library(tidyverse)
install.packages("psych")
library(psych)
install.packages("readr")
library(readr)
vdata <- read_csv("vdata.csv")
View(vdata)

# Set N/A values (i.e. values of 11) in HSQ equal to 1
vdata <- vdata %>% mutate_at(vars(starts_with("HSQ_")), ~ ifelse(. == 11, 1, .))

# Calculate a total HSQ score for each participant
vdata <- vdata %>% mutate(HSQ_total = rowSums(across(HSQ_1:HSQ_67)))

# Reverse code items 4, 5, 7, and 8 of PSS-10
reverse_columns <- c("PSS_4", "PSS_5", "PSS_7", "PSS_8")
vdata[, reverse_columns] <- 4 - vdata[, reverse_columns]

# Calculate a total PSS-10 score for each participant
vdata <- vdata %>% mutate(PSS_total = rowSums(across(PSS_1:PSS_10)))

# Calculate a total EQ-5D score for each participant
vdata <- vdata %>% mutate(EQ5D_total = rowSums(across(EQ5D_1:EQ5D_5)))

# Descriptive statistics
vdata %>% summarise(Mean = mean(HSQ_total),
                   SD = sd(HSQ_total),
                   Median = median(HSQ_total),
                   Minimum = min(HSQ_total),
                   Maximum = max(HSQ_total))
vdata %>% summarise(Mean = mean(age_c),
                   SD = sd(age_c),
                   Median = median(age_c),
                   Minimum = min(age_c),
                   Maximum = max(age_c))
vdata %>% summarise(Mean = mean(age_h),
                   SD = sd(age_h),
                   Median = median(age_h),
                   Minimum = min(age_h),
                   Maximum = max(age_h))
vdata %>% summarise(Mean = mean(stays_life, na.rm = T),
                   SD = sd(stays_life, na.rm = T),
                   Median = median(stays_life, na.rm = T),
                   Minimum = min(stays_life, na.rm = T),
                   Maximum = max(stays_life, na.rm = T))
vdata %>% summarise(Mean = mean(stays_year, na.rm = T),
                   SD = sd(stays_year, na.rm = T),
                   Median = median(stays_year, na.rm = T),
                   Minimum = min(stays_year, na.rm = T),
                   Maximum = max(stays_year, na.rm = T))
vdata %>% summarise(Mean = mean(LOS, na.rm = T),
                   SD = sd(LOS, na.rm = T),
                   Median = median(LOS, na.rm = T),
                   Minimum = min(LOS, na.rm = T),
                   Maximum = max(LOS, na.rm = T))
vdata %>% count(sex)
vdata %>% count(gender)
vdata %>% count(ethnicity)
vdata %>% count(education)
vdata %>% count(marital)
vdata %>% count(surgery)
vdata %>% count(planned)

# Create seperate dataset for just HSQ items
var_names <- paste0("HSQ_", 1:67)
HSQ_items <- select(vdata, all_of(var_names))
View(HSQ_items)

# Means and SDs of each HSQ item
means <- colMeans(HSQ_items)
sds <- apply(HSQ_items, 2, sd)
HSQ_MSD <- data.frame(Variable = names(means), Mean = round(means, 2), SD = round(sds, 2))
View(HSQ_MSD)

# Calculate convergent validity
cor.test(vdata$HSQ_total, vdata$PSS_total)

# Calculate divergent validity
cor.test(vdata$HSQ_total, vdata$EQ5D_total)

# Calculate internal consistency (Cronbachâ€™s alpha)
HSQ_alpha <- psych::alpha(HSQ_items)
print(HSQ_alpha)

# Display response frequencies as percentages
mrange <- as.data.frame(round((HSQ_alpha$response.freq * 100), 1))
View(mrange)

# Calculate inter-item correlations
rmatrix <- cor(HSQ_items, use = "complete.obs")
rmatrix <- as.data.frame(as.table(round(rmatrix, 2)))  ### table easier to read than 67x67 matrix
View(rmatrix)

# Create correlation matrix of inter-item correlations (note: this is a 67x67 matrix, too big for most screens)
ggplot(rmatrix, aes(Var1, Var2)) +
      geom_tile(aes(fill = Freq)) +
      geom_text(aes(label = Freq), size = 5, colour = "black") +
      scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
      theme_minimal()

# Identify any potentially redundant items (i.e. r > .8)
iic <- rmatrix %>% filter(Var1 != Var2)
iic_high <- iic %>% filter(Freq > .7)  ### no iic's > .8 so checked .7
View(iic_high)

# Calculate average inter-item correlations for each item
results_list <- list()
for (variable in var_names) {
  iic_variable <- iic %>% filter(Var2 == variable)
  results <- iic_variable %>% 
    summarise(
      Mean = round(mean(Freq), 2),
      SD = round(sd(Freq), 2),
      Median = median(Freq),
      Minimum = min(Freq),
      Maximum = max(Freq)
    )  
  results_list[[variable]] <- results
}
avg_iic <- bind_rows(results_list, .id = "Variable")
View(avg_iic)

# Identify items not correlating well with other items (i.e. < .3)
iic_low <- avg_iic %>% filter(Mean <= .3 & Median <= .3)
View(iic_low)

# Calculate test-retest
retest <- read_csv("retest_r.csv")
cor.test(retest$TOTAL1, retest$TOTAL2)

# Remove excluded items
HSQ_items2 <- HSQ_items[, -c(5, 13, 20, 22, 30, 36, 38, 42, 46, 47, 48, 53, 59, 60, 61, 62, 64, 65, 66, 67)]

### Exploratory Factor Analysis ###
install.packages("REdaS")
library(REdaS)

# Bartlett's and KMO
bart_spher(HSQ_items2)
KMOS(HSQ_items2)

# Parallel Analysis, Scree plot, and Eigenvalues
nfactors <- psych::fa.parallel(HSQ_items2, fm = "ml", fa = "fa")
nfactors$fa.values

# Minimum Average Partial (MAP) test
VSS(HSQ_items2, n = 7, rotate = "oblimin", fm = "mle")

# EFA for models with 3, 4, and 6 factors
HSQ_f6 <- psych::fa(HSQ_items2, nfactors = 6, rotate = "oblimin", fm = "ml")
1 - ((HSQ_f6$STATISTIC - HSQ_f6$dof)/(HSQ_f6$null.chisq - HSQ_f6$null.dof))  # Calculate CFI
print(HSQ_f6)
HSQ_f3 <- psych::fa(HSQ_items2, nfactors = 3, rotate = "oblimin", fm = "ml")
1 - ((HSQ_f3$STATISTIC - HSQ_f3$dof)/(HSQ_f3$null.chisq - HSQ_f3$null.dof))  # Calculate CFI
print(HSQ_f3)
HSQ_f4 <- psych::fa(HSQ_items2, nfactors = 4, rotate = "oblimin", fm = "ml")
1 - ((HSQ_f3$STATISTIC - HSQ_f3$dof)/(HSQ_f3$null.chisq - HSQ_f3$null.dof))  # Calculate CFI
print(HSQ_f4)

# Exclude items that don't load >=.4 onto 4-factor model
HSQ_items3 <- HSQ_items2[, -c(7, 8, 9, 19, 20, 22, 33, 37, 39, 43, 44, 47)]
View(HSQ_items3)

# Re-run model without bad items
HSQ_f4b <- psych::fa(HSQ_items3, nfactors = 4, rotate = "oblimin", fm = "ml")
1 - ((HSQ_f4b$STATISTIC - HSQ_f4b$dof)/(HSQ_f4b$null.chisq - HSQ_f4b$null.dof))  # Calculate CFI
print(HSQ_f4b)

# Exclude items that don't load >=.4 onto new 4-factor model
HSQ_items4 <- HSQ_items3[, -c(8, 20, 26)]
View(HSQ_items4)

# Re-run model without bad items
HSQ_f4c <- psych::fa(HSQ_items4, nfactors = 4, rotate = "oblimin", fm = "ml")
1 - ((HSQ_f4c$STATISTIC - HSQ_f4c$dof)/(HSQ_f4c$null.chisq - HSQ_f4c$null.dof))  # Calculate CFI
print(HSQ_f4c)

# Exclude items that don't load >=.4 onto new 4-factor model
HSQ_items5 <- HSQ_items4[, -c(8, 31)]
View(HSQ_items5)

# Re-run model without bad items
HSQ_f4d <- psych::fa(HSQ_items5, nfactors = 4, rotate = "oblimin", fm = "ml")
1 - ((HSQ_f4d$STATISTIC - HSQ_f4d$dof)/(HSQ_f4d$null.chisq - HSQ_f4d$null.dof))  # Calculate CFI
print(HSQ_f4d)
